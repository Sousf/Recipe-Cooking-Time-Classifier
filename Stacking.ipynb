{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "senior-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-stylus",
   "metadata": {},
   "source": [
    "### TRY UNDER FITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "sitting-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in sparse matrix data and transforming.\n",
    "\n",
    "def Countvec_digestSparse(file):\n",
    "    x_train_original = pd.read_csv(file, index_col = False, delimiter = ',', header=0)\n",
    "    oversample = RandomUnderSampler(sampling_strategy='majority')\n",
    "\n",
    "\n",
    "    # First random oversampling, bring count of class 3 to be equal to the highest class count.\n",
    "    X_oversampled, y_oversampled = oversample.fit_resample(x_train_original, x_train_original.loc[:,'duration_label'])\n",
    "\n",
    "\n",
    "    # Second random oversampling, bring the count of class 2 to be equal to the other 2 classes\n",
    "    X_oversampled, y_oversampled = oversample.fit_resample(X_oversampled, y_oversampled)\n",
    "\n",
    "    # .value_counts()\n",
    "    # use recipe name as an example\n",
    "    train_corpus = X_oversampled.loc[:,['name','steps','ingredients']]\n",
    "    train_corpus['steps'] = train_corpus['steps'].apply(eval)\n",
    "    train_corpus['ingredients'] = train_corpus['ingredients'].apply(eval)\n",
    "    train_corpus['steps'] = train_corpus['steps'].apply(' '.join)\n",
    "    train_corpus['ingredients'] = train_corpus['ingredients'].apply(' '.join)\n",
    "    \n",
    "    \n",
    "    all_words = []\n",
    "\n",
    "    for i in range (0, len(train_corpus['steps'])):\n",
    "        s = ''\n",
    "        s += train_corpus.loc[i,'name'] + train_corpus.loc[i, 'steps'] + train_corpus.loc[i, 'ingredients']\n",
    "        all_words.append(s)\n",
    "        \n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features = 2000)\n",
    "    X = vectorizer.fit(all_words)\n",
    "    vocab_dict = vectorizer.vocabulary_\n",
    "    # vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "    X_final = vectorizer.transform(all_words)\n",
    "    \n",
    "    return X_final, y_oversampled\n",
    "\n",
    "    \n",
    "\n",
    "X_final, y_oversampled = Countvec_digestSparse('recipe_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "soviet-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def digestDoc2Vec():\n",
    "   # Doc2Vect is a technique to transfer words into numerical representation. \n",
    "    # https://www.shibumi-ai.com/post/a-gentle-introduction-to-doc2vec\n",
    "    d2v_ingr = pd.read_csv(\"recipe_text_features_doc2vec100/train_ingr_doc2vec100.csv\", header=None)\n",
    "    d2v_name = pd.read_csv(\"recipe_text_features_doc2vec100/train_name_doc2vec100.csv\", header=None)\n",
    "    d2v_steps = pd.read_csv(\"recipe_text_features_doc2vec100/train_steps_doc2vec100.csv\", header=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Extract class_labels from training set \n",
    "    # quick = 1\n",
    "    # medium = 2\n",
    "    # slow = 3\n",
    "    data_train = pd.read_csv('recipe_train.csv')\n",
    "\n",
    "    # Obtain the labels\n",
    "    train_label = data_train.iloc[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Feature selection with f_classif (ANOVA F-value)\n",
    "    # ANOVA f-value shows how well a feature discriminate between classes\n",
    "    # The more discrimination, the better that feature is in predicting the class label.\n",
    "    # d2v_name_new = SelectKBest(k=90).fit_transform(d2v_name, train_label)\n",
    "    # d2v_ingr_new = SelectKBest(k=90).fit_transform(d2v_ingr, train_label)\n",
    "    # d2v_steps_new = SelectKBest(k=90).fit_transform(d2v_steps, train_label)\n",
    "    d2v_name_new = pd.DataFrame(d2v_name)\n",
    "    d2v_ingr_new = pd.DataFrame(d2v_ingr)\n",
    "    d2v_steps_new = pd.DataFrame(d2v_steps)\n",
    "    # print(d2v_ingr_new)\n",
    "    # print(d2v_steps_new)\n",
    "    # print(d2v_name_new.shape)\n",
    "\n",
    "\n",
    "    # Create a new dataframe of data, but this time, name, steps and ingr has been engineered to have doc2vec features.\n",
    "    # 100 doc2vec features were given, but we selected 20 best features using ANOVA f-value.\n",
    "    f_data = d2v_name_new.join(data_train.iloc[:,1:3], on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "    f_data = d2v_name_new.join(d2v_steps_new, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "    f_data = f_data.join(d2v_ingr_new, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "\n",
    "    # Standardise the data so that the mean is 0\n",
    "    scaler = StandardScaler()\n",
    "    f_data = scaler.fit_transform(f_data)\n",
    "\n",
    "    # normalise all values to be between 0 and 1\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    f_data = minmax_scaler.fit_transform(f_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Oversampling, because the distribution of classes in training data is highly skewed towards quick and medium.\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "    # First random oversampling, bring count of class 3 to be equal to the highest class count.\n",
    "    X_oversampled, y_oversampled = oversample.fit_resample(f_data, train_label)\n",
    "\n",
    "    # Second random oversampling, bring the count of class 2 to be equal to the other 2 classes\n",
    "    X_oversampled, y_oversampled = oversample.fit_resample(X_oversampled, y_oversampled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Now we should have 20,246 instances for each class.\n",
    "    # Splitting the provided training into its own train/test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, test_size=0.2, stratify=y_oversampled, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # X = pd.DataFrame(X_oversampled)\n",
    "    # y = pd.DataFrame(y_oversampled)\n",
    "\n",
    "    # full = X.join(y, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "    return X_oversampled, y_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "remarkable-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oversampled, y_oversampled = digestDoc2Vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-choice",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "attended-banks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.6214191636483372\n",
      "KNN Bagging Accuracy: 0.5859400724399078\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, test_size=0.2, stratify=y_oversampled, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=10)\n",
    "bagging = BaggingClassifier(base_estimator=KNN,n_estimators=10,\n",
    "                            max_samples=0.5, max_features=0.5, n_jobs=-1,random_state=0)\n",
    "\n",
    "KNN.fit(X_train,y_train)\n",
    "bagging.fit(X_train,y_train)\n",
    "print(\"KNN:\",KNN.score(X_test,y_test))\n",
    "print(\"KNN Bagging Accuracy:\",bagging.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "identical-capitol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT test: 0.3829268292682927\n",
      "DT train: 0.39170225747406956\n",
      "DT Bagging test Accuracy: 0.7056910569105691\n",
      "DT Bagging train Accuracy: 0.7470002033760423\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_oversampled, test_size=0.2, stratify=y_oversampled, random_state=42)\n",
    "\n",
    "\n",
    "DT = DecisionTreeClassifier(max_depth=6, criterion='entropy', max_features='log2',random_state=0)\n",
    "bagging2 = BaggingClassifier(base_estimator=DT,n_estimators=100,\n",
    "                            max_samples=0.5, max_features=0.5, n_jobs=-1,random_state=0)\n",
    "\n",
    "\n",
    "DT.fit(X_train,y_train)\n",
    "bagging2.fit(X_train,y_train)\n",
    "print(\"DT test:\",DT.score(X_test,y_test))\n",
    "print(\"DT train:\",DT.score(X_train,y_train))\n",
    "print(\"DT Bagging test Accuracy:\",bagging2.score(X_test,y_test))\n",
    "print(\"DT Bagging train Accuracy:\",bagging2.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dependent-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7772357723577236\n",
      "0.8759406141956477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.77      0.75       410\n",
      "         2.0       0.74      0.71      0.73       410\n",
      "         3.0       0.87      0.85      0.86       410\n",
      "\n",
      "    accuracy                           0.78      1230\n",
      "   macro avg       0.78      0.78      0.78      1230\n",
      "weighted avg       0.78      0.78      0.78      1230\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[316,  74,  20],\n",
       "       [ 87, 292,  31],\n",
       "       [ 34,  28, 348]], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_oversampled, test_size=0.2, stratify=y_oversampled, random_state=42)\n",
    "\n",
    "\n",
    "LinearSVC_clf = svm.LinearSVC(dual=False, multi_class='ovr', random_state=0)\n",
    "SVC_clf = make_pipeline(StandardScaler(with_mean=False), MaxAbsScaler(), LinearSVC_clf)\n",
    "forest = RandomForestClassifier(n_estimators=500, max_features='log2', max_depth=2, \n",
    "                                criterion='entropy', n_jobs = -1, random_state = 0)\n",
    "estimators = [\n",
    "     ('bagging', bagging2),\n",
    "     ('CountVecSVC', SVC_clf),\n",
    "      ('Zero_R', DummyClassifier(strategy='most_frequent')),\n",
    "      ('Boosting',AdaBoostClassifier(n_estimators=100, random_state=0))]\n",
    "\n",
    "\n",
    "clf = StackingClassifier(\n",
    "     estimators=estimators, final_estimator=SVC_clf, n_jobs=6\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_test_predict_stack = clf.predict(X_test)\n",
    "y_train_predict_stack = clf.predict(X_train)\n",
    "print(accuracy_score(y_test, y_test_predict_stack))\n",
    "print(accuracy_score(y_train, y_train_predict_stack))\n",
    "print(classification_report(y_test, y_test_predict_stack))\n",
    "cm = confusion_matrix(y_test, y_test_predict_stack)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "facial-repair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "final_test = scipy.sparse.load_npz('CountVec.npz')\n",
    "\n",
    "# stacking_clf.fit(X_train, y_train)\n",
    "real_test_pred = clf.predict(final_test)\n",
    "# final_test.shape\n",
    "\n",
    "a = [x for x in range(1,10001)]\n",
    "\n",
    "result = {\n",
    "    'id': a,\n",
    "    'duration_label': real_test_pred\n",
    "}\n",
    "result = pd.DataFrame(result)\n",
    "result.to_csv('result.csv', index = False)\n",
    "real_test_pred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
