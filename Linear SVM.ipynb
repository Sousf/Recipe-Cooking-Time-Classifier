{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "hawaiian-criticism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44111683, 0.0617284 , 0.02564103, ..., 0.51126455, 0.37345331,\n",
       "        0.3947126 ],\n",
       "       [0.3907844 , 0.0617284 , 0.12820513, ..., 0.4447808 , 0.46763176,\n",
       "        0.52414391],\n",
       "       [0.43168394, 0.04938272, 0.25641026, ..., 0.33210021, 0.47459023,\n",
       "        0.34165161],\n",
       "       ...,\n",
       "       [0.56981364, 0.05296908, 0.2597055 , ..., 0.50625011, 0.50360741,\n",
       "        0.33506021],\n",
       "       [0.5161363 , 0.21205649, 0.36047591, ..., 0.48089585, 0.44863147,\n",
       "        0.26215384],\n",
       "       [0.5430582 , 0.14418386, 0.19179803, ..., 0.40271453, 0.65661782,\n",
       "        0.49141788]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import load\n",
    "import pickle\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "# Look into partial fitting. \n",
    "# Dataset is not evenly distributed with most being 1.\n",
    "# Dummy Classifier has 50 % accuracy score... (Only give instances the most frequent label)\n",
    "\n",
    "\n",
    "\n",
    "# steps_countvec = scipy.sparse.load_npz('recipe_text_features_countvec/train_steps_countvectorizer.pkl')\n",
    "\n",
    "# Doc2Vect is a technique to transfer words into numerical representation. \n",
    "# https://www.shibumi-ai.com/post/a-gentle-introduction-to-doc2vec\n",
    "d2v_ingr = pd.read_csv(\"../data/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_ingr_doc2vec100.csv\", header=None)\n",
    "d2v_name = pd.read_csv(\"../data/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_name_doc2vec100.csv\", header=None)\n",
    "d2v_steps = pd.read_csv(\"../data/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_steps_doc2vec100.csv\", header=None)\n",
    "\n",
    "\n",
    "\n",
    "# Extract class_labels from training set \n",
    "# quick = 1\n",
    "# medium = 2\n",
    "# slow = 3\n",
    "data_train = pd.read_csv('../data/COMP30027_2021_Project2_datasets/recipe_train.csv')\n",
    "\n",
    "# Obtain the labels\n",
    "train_label = data_train.iloc[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Feature selection with f_classif (ANOVA F-value)\n",
    "# ANOVA f-value shows how well a feature discriminate between classes\n",
    "# The more discrimination, the better that feature is in predicting the class label.\n",
    "# d2v_name_new = SelectKBest(k=90).fit_transform(d2v_name, train_label)\n",
    "# d2v_ingr_new = SelectKBest(k=90).fit_transform(d2v_ingr, train_label)\n",
    "# d2v_steps_new = SelectKBest(k=90).fit_transform(d2v_steps, train_label)\n",
    "d2v_name_new = pd.DataFrame(d2v_name)\n",
    "d2v_ingr_new = pd.DataFrame(d2v_ingr)\n",
    "d2v_steps_new = pd.DataFrame(d2v_steps)\n",
    "# print(d2v_ingr_new)\n",
    "# print(d2v_steps_new)\n",
    "# print(d2v_name_new.shape)\n",
    "\n",
    "\n",
    "# Create a new dataframe of data, but this time, name, steps and ingr has been engineered to have doc2vec features.\n",
    "# 100 doc2vec features were given, but we selected 20 best features using ANOVA f-value.\n",
    "f_data = d2v_name_new.join(data_train.iloc[:,1:3], on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "f_data = f_data.join(d2v_steps_new, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "f_data = f_data.join(d2v_ingr_new, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "# # Standardise the data so that the mean is 0\n",
    "# scaler = StandardScaler()\n",
    "# f_data = scaler.fit_transform(f_data)\n",
    "\n",
    "# normalise all values to be between 0 and 1\n",
    "minmax_scaler = MinMaxScaler()\n",
    "f_data = minmax_scaler.fit_transform(f_data)\n",
    "\n",
    "\n",
    "X_new = SelectKBest(chi2, k=20).fit_transform(f_data, train_label)\n",
    "\n",
    "# Splitting the provided training into its own train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, train_label, test_size=0.2, stratify=train_label, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "smX, smY = smote.fit_resample(X_train, y_train)\n",
    "# # Oversampling, because the distribution of classes in training data is highly skewed towards quick and medium.\n",
    "# oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "# # First random oversampling, bring count of class 3 to be equal to the highest class count.\n",
    "# X_oversampled, y_oversampled = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Second random oversampling, bring the count of class 2 to be equal to the other 2 classes\n",
    "# X_oversampled, y_oversampled = oversample.fit_resample(X_oversampled, y_oversampled)\n",
    "\n",
    "\n",
    "\n",
    "# X_oversampled = pd.DataFrame(X_oversampled)\n",
    "# y_oversampled = pd.DataFrame(y_oversampled)\n",
    "# X_oversampled.to_csv('X_oversampled', index = False)\n",
    "# y_oversampled.to_csv('y_oversampled', index = False)\n",
    "\n",
    "\n",
    "# X = pd.DataFrame(X_oversampled)\n",
    "# y = pd.DataFrame(y_oversampled)\n",
    "\n",
    "# full = X.join(y, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "smX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "roman-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot learning curve\n",
    "\n",
    "\n",
    "# LinearSVC = svm.LinearSVC(dual=False, multi_class='ovr', random_state=0)\n",
    "# SVC_clf = make_pipeline(MinMaxScaler(), LinearSVC)\n",
    "SVC_clf = svm.LinearSVC(dual=False, multi_class='ovr', random_state=0)\n",
    "# plot_learning_curves(smX, smY, X_test, y_test, SVC_clf, scoring= 'accuracy')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cultural-payment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0.566375\n",
      "train 0.5926817723446729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.66      0.64      0.65      3541\n",
      "         2.0       0.73      0.50      0.59      4049\n",
      "         3.0       0.14      0.60      0.22       410\n",
      "\n",
      "    accuracy                           0.57      8000\n",
      "   macro avg       0.51      0.58      0.49      8000\n",
      "weighted avg       0.67      0.57      0.60      8000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2264,  685,  592],\n",
       "       [1064, 2022,  963],\n",
       "       [  93,   72,  245]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing LinearSVM from sklearn library\n",
    "# Similar to SVC with parameter kernel=’linear’, \n",
    "# scale better to large numbers of samples.\n",
    "# supports both dense and sparse input\n",
    "# multiclass support is handled according to a one-vs-the-rest scheme.\n",
    "# The more features the better\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# LinearSVC = svm.LinearSVC(dual=False, multi_class='ovr', random_state=0)\n",
    "\n",
    "SVC_clf.fit(smX, smY)\n",
    "\n",
    "y_test_predict = SVC_clf.predict(X_test)\n",
    "y_train_predict = SVC_clf.predict(smX)\n",
    "# np.save('y_predict_name_sparse.npy', y_test_predict)\n",
    "print(\"test\", accuracy_score(y_test, y_test_predict))\n",
    "print(\"train\", accuracy_score(smY, y_train_predict))\n",
    "# print(LinearSVC_clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 0 score for precision, recall and f1 on class label 3...\n",
    "print(classification_report(y_test, y_test_predict))\n",
    "cm = confusion_matrix(y_test, y_test_predict)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation To evaluate model\n",
    "# Shuffle Split is a random split method, not guranteed unique split but likely on large data.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "scores = cross_val_score(LinearSVC_clf, X_oversampled, y_oversampled, cv=cv)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf_SGD = SGDClassifier(max_iter=1000, tol=1e-3, n_jobs=6, random_state=0)\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, clf_SGD, scoring= 'accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
