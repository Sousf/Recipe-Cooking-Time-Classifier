{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "played-sunday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60738, 20)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "# Look into partial fitting. \n",
    "# Dataset is not evenly distributed with most being 1.\n",
    "# Dummy Classifier has 50 % accuracy score... (Only give instances the most frequent label)\n",
    "\n",
    "\n",
    "# steps_countvec = scipy.sparse.load_npz('recipe_text_features_countvec/train_steps_countvectorizer.pkl')\n",
    "\n",
    "# Doc2Vect is a technique to transfer words into numerical representation. \n",
    "# https://www.shibumi-ai.com/post/a-gentle-introduction-to-doc2vec\n",
    "d2v_ingr = pd.read_csv(\"recipe_text_features_doc2vec100/train_ingr_doc2vec100.csv\", header=None)\n",
    "d2v_name = pd.read_csv(\"recipe_text_features_doc2vec100/train_name_doc2vec100.csv\", header=None)\n",
    "d2v_steps = pd.read_csv(\"recipe_text_features_doc2vec100/train_steps_doc2vec100.csv\", header=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract class_labels from training set \n",
    "# quick = 1\n",
    "# medium = 2\n",
    "# slow = 3\n",
    "data_train = pd.read_csv('recipe_train.csv')\n",
    "\n",
    "# Obtain the labels\n",
    "train_label = data_train.iloc[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Feature selection with f_classif (ANOVA F-value)\n",
    "# ANOVA f-value shows how well a feature discriminate between classes\n",
    "# The more discrimination, the better that feature is in predicting the class label.\n",
    "# d2v_name_new = SelectKBest(k=90).fit_transform(d2v_name, train_label)\n",
    "# d2v_ingr_new = SelectKBest(k=90).fit_transform(d2v_ingr, train_label)\n",
    "# d2v_steps_new = SelectKBest(k=90).fit_transform(d2v_steps, train_label)\n",
    "d2v_name_new = pd.DataFrame(d2v_name)\n",
    "d2v_ingr_new = pd.DataFrame(d2v_ingr)\n",
    "d2v_steps_new = pd.DataFrame(d2v_steps)\n",
    "# print(d2v_ingr_new)\n",
    "# print(d2v_steps_new)\n",
    "# print(d2v_name_new.shape)\n",
    "\n",
    "\n",
    "tot = []\n",
    "for i, el in enumerate(data_train.iloc[:,1]):\n",
    "    total = el+data_train.iloc[i,2]\n",
    "    tot.append(total)\n",
    "    \n",
    "    \n",
    "tot = pd.DataFrame(tot)\n",
    "\n",
    "# Create a new dataframe of data, but this time, name, steps and ingr has been engineered to have doc2vec features.\n",
    "# 100 doc2vec features were given, but we selected 20 best features using ANOVA f-value.\n",
    "f_data = d2v_name_new.join(tot, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "f_data = f_data.join(d2v_steps_new, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "f_data = f_data.join(d2v_ingr_new, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "\n",
    "# Standardise the data so that the mean is 0\n",
    "scaler = StandardScaler()\n",
    "f_data = scaler.fit_transform(f_data)\n",
    "\n",
    "# normalise all values to be between 0 and 1\n",
    "minmax_scaler = MinMaxScaler()\n",
    "f_data = minmax_scaler.fit_transform(f_data)\n",
    "   \n",
    "f_data = SelectKBest(chi2, k=20).fit_transform(f_data, train_label)\n",
    "\n",
    "\n",
    "# Oversampling, because the distribution of classes in training data is highly skewed towards quick and medium.\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "# First random oversampling, bring count of class 3 to be equal to the highest class count.\n",
    "X_oversampled, y_oversampled = oversample.fit_resample(f_data, train_label)\n",
    "\n",
    "# Second random oversampling, bring the count of class 2 to be equal to the other 2 classes\n",
    "X_oversampled, y_oversampled = oversample.fit_resample(X_oversampled, y_oversampled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now we should have 20,246 instances for each class.\n",
    "# Splitting the provided training into its own train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, test_size=0.2, stratify=y_oversampled, random_state=42)\n",
    "\n",
    "X_oversampled.shape\n",
    "# tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "julian-publication",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_corpus_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-ebc2c13556af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# tokenize a training corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mcorpus_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_corpus_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# train doc2vec on the training corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_corpus_name' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "obvious-proposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6689990121830754\n",
      "0.7249639843589216\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.63      0.64      0.64      4049\n",
      "         2.0       0.70      0.42      0.52      4049\n",
      "         3.0       0.68      0.95      0.79      4050\n",
      "\n",
      "    accuracy                           0.67     12148\n",
      "   macro avg       0.67      0.67      0.65     12148\n",
      "weighted avg       0.67      0.67      0.65     12148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# pca = PCA(n_components = 2)\n",
    "# pca.fit(X_oversampled)\n",
    "# pca_x=pca.transform(X_oversampled)\n",
    "# # test = pd.DataFrame(pca.components_)\n",
    "\n",
    "\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(pca_x, y_oversampled, test_size=0.2, stratify=y_oversampled, random_state=42)\n",
    "\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "knn_predict = knn_clf.predict(X_test)\n",
    "knn_predict_train = knn_clf.predict(X_train)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_test, knn_predict))\n",
    "print(accuracy_score(y_train, knn_predict_train))\n",
    "print(classification_report(y_test, knn_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
