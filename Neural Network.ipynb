{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "chicken-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from numpy import load\n",
    "import pickle\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sized-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nnet(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, num_classes):\n",
    "        super(Nnet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden2_size, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Second Layer\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Final layer\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "wired-highway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nnet(\n",
      "  (fc1): Linear(in_features=4, out_features=100, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Nnet(4,100,50,3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "metropolitan-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "affiliated-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look into partial fitting. \n",
    "# Dataset is not evenly distributed with most being 1.\n",
    "# Dummy Classifier has 50 % accuracy score... (Only give instances the most frequent label)\n",
    "\n",
    "\n",
    "\n",
    "# steps_countvec = scipy.sparse.load_npz('recipe_text_features_countvec/train_steps_countvectorizer.pkl')\n",
    "\n",
    "# Doc2Vect is a technique to transfer words into numerical representation. \n",
    "# https://www.shibumi-ai.com/post/a-gentle-introduction-to-doc2vec\n",
    "d2v_ingr = pd.read_csv(\"../data/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_ingr_doc2vec100.csv\", header=None)\n",
    "d2v_name = pd.read_csv(\"../data/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_name_doc2vec100.csv\", header=None)\n",
    "d2v_steps = pd.read_csv(\"../data/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_steps_doc2vec100.csv\", header=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract class_labels from training set \n",
    "# quick = 1\n",
    "# medium = 2\n",
    "# slow = 3\n",
    "data_train = pd.read_csv('../data/COMP30027_2021_Project2_datasets/recipe_train.csv')\n",
    "\n",
    "# Obtain the labels\n",
    "train_label = data_train.iloc[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Feature selection with f_classif (ANOVA F-value)\n",
    "# ANOVA f-value shows how well a feature discriminate between classes\n",
    "# The more discrimination, the better that feature is in predicting the class label.\n",
    "# d2v_name_new = SelectKBest(k=90).fit_transform(d2v_name, train_label)\n",
    "# d2v_ingr_new = SelectKBest(k=90).fit_transform(d2v_ingr, train_label)\n",
    "# d2v_steps_new = SelectKBest(k=90).fit_transform(d2v_steps, train_label)\n",
    "d2v_name_new = pd.DataFrame(d2v_name)\n",
    "d2v_ingr_new = pd.DataFrame(d2v_ingr)\n",
    "d2v_steps_new = pd.DataFrame(d2v_steps)\n",
    "# print(d2v_ingr_new)\n",
    "# print(d2v_steps_new)\n",
    "# print(d2v_name_new.shape)\n",
    "\n",
    "\n",
    "# Create a new dataframe of data, but this time, name, steps and ingr has been engineered to have doc2vec features.\n",
    "# 100 doc2vec features were given, but we selected 20 best features using ANOVA f-value.\n",
    "f_data = d2v_name_new.join(data_train.iloc[:,1:3], on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "f_data = f_data.join(d2v_steps_new, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "f_data = f_data.join(d2v_ingr_new, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "\n",
    "# # Standardise the data so that the mean is 0\n",
    "# scaler = StandardScaler()\n",
    "# f_data = scaler.fit_transform(f_data)\n",
    "\n",
    "# normalise all values to be between 0 and 1\n",
    "minmax_scaler = MinMaxScaler()\n",
    "f_data = minmax_scaler.fit_transform(f_data)\n",
    "\n",
    "\n",
    "x = f_data\n",
    "y = train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-consensus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-receiver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-sweden",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-danish",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
