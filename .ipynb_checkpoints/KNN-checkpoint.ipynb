{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "played-sunday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48591, 20), (48591,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Look into partial fitting. \n",
    "# Dataset is not evenly distributed with most being 1.\n",
    "# Dummy Classifier has 50 % accuracy score... (Only give instances the most frequent label)\n",
    "\n",
    "\n",
    "# steps_countvec = scipy.sparse.load_npz('recipe_text_features_countvec/train_steps_countvectorizer.pkl')\n",
    "\n",
    "# Doc2Vect is a technique to transfer words into numerical representation. \n",
    "# https://www.shibumi-ai.com/post/a-gentle-introduction-to-doc2vec\n",
    "d2v_ingr = pd.read_csv(\"../data/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_ingr_doc2vec100.csv\", header=None)\n",
    "d2v_name = pd.read_csv(\"../data/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_name_doc2vec100.csv\", header=None)\n",
    "d2v_steps = pd.read_csv(\"../data/COMP30027_2021_Project2_datasets/recipe_text_features_doc2vec100/train_steps_doc2vec100.csv\", header=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract class_labels from training set \n",
    "# quick = 1\n",
    "# medium = 2\n",
    "# slow = 3\n",
    "data_train = pd.read_csv('../data/COMP30027_2021_Project2_datasets/recipe_train.csv')\n",
    "\n",
    "# Obtain the labels\n",
    "train_label = data_train.iloc[:,-1]\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Feature selection with f_classif (ANOVA F-value)\n",
    "# ANOVA f-value shows how well a feature discriminate between classes\n",
    "# The more discrimination, the better that feature is in predicting the class label.\n",
    "# d2v_name_new = SelectKBest(k=90).fit_transform(d2v_name, train_label)\n",
    "# d2v_ingr_new = SelectKBest(k=90).fit_transform(d2v_ingr, train_label)\n",
    "# d2v_steps_new = SelectKBest(k=90).fit_transform(d2v_steps, train_label)\n",
    "d2v_name_new = pd.DataFrame(d2v_name)\n",
    "d2v_ingr_new = pd.DataFrame(d2v_ingr)\n",
    "d2v_steps_new = pd.DataFrame(d2v_steps)\n",
    "# print(d2v_ingr_new)\n",
    "# print(d2v_steps_new)\n",
    "# print(d2v_name_new.shape)\n",
    "\n",
    "\n",
    "tot = []\n",
    "for i, el in enumerate(data_train.iloc[:,1]):\n",
    "    total = el+data_train.iloc[i,2]\n",
    "    tot.append(total)\n",
    "    \n",
    "    \n",
    "tot = pd.DataFrame(tot)\n",
    "\n",
    "# Create a new dataframe of data, but this time, name, steps and ingr has been engineered to have doc2vec features.\n",
    "# 100 doc2vec features were given, but we selected 20 best features using ANOVA f-value.\n",
    "f_data = d2v_name_new.join(tot, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "f_data = f_data.join(d2v_steps_new, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "f_data = f_data.join(d2v_ingr_new, on=None, how='left', lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "\n",
    "# Standardise the data so that the mean is 0 \n",
    "scaler = StandardScaler()\n",
    "f_data = scaler.fit_transform(f_data)\n",
    "\n",
    "# normalise all values to be between 0 and 1 for chi2\n",
    "minmax_scaler = MinMaxScaler()\n",
    "f_data = minmax_scaler.fit_transform(f_data)\n",
    "   \n",
    "# Feature selection for chi2\n",
    "f_data = SelectKBest(chi2, k=20).fit_transform(f_data, train_label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now we should have 20,246 instances for each class.\n",
    "# Splitting the provided training into its own train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(f_data, train_label, test_size=0.2, stratify=train_label, random_state=42)\n",
    "\n",
    "\n",
    "# Oversampling, because the distribution of classes in training data is highly skewed towards quick and medium.\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "# First random oversampling, bring count of class 3 to be equal to the highest class count.\n",
    "X_oversampled, y_oversampled = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# Second random oversampling, bring the count of class 2 to be equal to the other 2 classes\n",
    "X_oversampled, y_oversampled = oversample.fit_resample(X_oversampled, y_oversampled)\n",
    "\n",
    "\n",
    "\n",
    "smX, smY = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "smX.shape, smY.shape\n",
    "X_oversampled.shape, y_oversampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "obvious-proposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0.494\n",
      "train 0.7081352513840011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.61      0.59      0.60      3541\n",
      "         2.0       0.71      0.40      0.51      4049\n",
      "         3.0       0.11      0.61      0.19       410\n",
      "\n",
      "    accuracy                           0.49      8000\n",
      "   macro avg       0.48      0.53      0.43      8000\n",
      "weighted avg       0.63      0.49      0.53      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# pca = PCA(n_components = 2)\n",
    "# pca.fit(X_oversampled)\n",
    "# pca_x=pca.transform(X_oversampled)\n",
    "# # test = pd.DataFrame(pca.components_)\n",
    "\n",
    "\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(pca_x, y_oversampled, test_size=0.2, stratify=y_oversampled, random_state=42)\n",
    "\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_clf.fit(smX, smY)\n",
    "\n",
    "\n",
    "knn_predict = knn_clf.predict(X_test)\n",
    "knn_predict_train = knn_clf.predict(smX)\n",
    "\n",
    "\n",
    "print(\"test\", accuracy_score(y_test, knn_predict))\n",
    "print(\"train\", accuracy_score(smY, knn_predict_train))\n",
    "print(classification_report(y_test, knn_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
